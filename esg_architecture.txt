L&G ESG Scope 3
Workflow and agentic AI
functional architecture
July 2025

th
Extract from 8 August session

Data

Investment

Climate and nature

Reporting

Business need exploration with L&G
Leveraging Kyndryl’s Agentic AI Framework (a self organising, dynamic system) to address individual use cases across the
Institutional Retirement pricing, climate risk and nature modelling, and sustainable investment
AI-driven economic model definition,
orchestration, specification and simulation on how
shifts in technology, policy, reputation, market and
customer behavior preferences impact companies,
especially based on their emissions profiles

Going beyond the basics on
addressing the data quality issues,
estimations and imputation of
missing values

Agentic AI as integration
layer(‘glue’) across multiple
fragmented systems, removing
the need for data centralization

Federated Data
Collection and
Aggregation

Using AI on satellite data, sensor
feeds, climate model outputs to
identify patterns and provide highresolution risk maps for assets

Dynamic
Transition Risk
Simulation
AI Powered
Data Quality
Improvement

Curation of internal and external
data into unique knowledge &
data assets (e.g. for peer
benchmarking, pricing, portfolio
risk management etc)

Anomaly
Detection

Timely detection of outliers and
anomalies in the raw data feeds
(internal and external), portfolio
level data for greater trust, better
risk management and early
identification of

High-Resolution
Physical Risk
Modeling
Portfolio
Optimization
with Emission
Constraints

Data and
Knowledge
Asset Curation

Illustrative and not exhaustive,
to be explored based on Legal
& General priorities, pain
points and opportunities

AI - Driven
Reporting and
Compliance

Efficient and effective compliance
with Scope 3 regulations,
including horizon scanning of
regulatory evolution

Dynamic Pricing
& Yield
Calculation

AI optimization algorithms to
recommend portfolio adjustments
that meet return objectives while
minimising financed emissions
(optimization under constraint)

Agentic AI determination of climate factors affect asset
valuation (e.g. learn correlation between market data
and Scope 3 exposure to better understand volatility,
yields, credit risk and price in appropriately)

High certainty and can be mocked up with
relative ease

Data collection and regulatory
reporting workflow (end to end)

Data

Investment

Climate and nature

Reporting

Scope 3 as Part of ESRS Regulatory Reporting Workflow 1 of 6
End to End Workflow Perspective
Scope 3 Data Acquisition

Emissions Calculation

•
•
•

•
•

Identify Relevant Categories
Internal Data Sourcing
External Data Sourcing

Apply Emission Factors
Category Aggregation

Reporting & Publication
•
•

Scope 3
Data
Acquisition

Emissions
Calculation
Data
Preparation
&
Validation

Disclosure Preparation
Integration into Annual Report /
Sustainability Report

Reporting &
Publication
Assurance &
Audit Trail

Data Preparation & Validation
•
•
•

Data Cleaning & Harmonisation
Data Gap Identification
Data Validation

Data Preparation & Validation
•
•

Internal Review
External Assurance Support

Agentic AI Functional Design
Data Acquisition Agents: Handle
ERP/API integrations, interface to
supplier portals etc
Data Validation Agents: Run
harmonisation, QA, anomaly
detection
Emission Calculation Agents: Map to
Emission Factors (EFs), perform
category aggregation

Assurance Agents: Compile
evidence packs, track lineage
Reporting Agents: Format, tag, and
publish disclosure

Scope 3 as Part of ESRS Regulatory Reporting Workflow 2 of 6
Level 0: Scope 3 Data Acquisition
Level 1

Identify Relevant
Categories

Level 2

Inputs

Outputs

Data & Business Logic

Illustrative Data Feeds

Category mapping to ESRS
GHG Protocol

- Double materiality
matrix (internal)
- Previous year’s Scope 3
mapping (internal)
- GHG Protocol guidance
(external)

List of applicable Scope 3
categories with ESRS
(based on IG3 list of
metrics and similar)

Map internal materiality
results to GHG Protocol
categories, align with ESRS
E1 requirements

- Internal ESG materiality
output
- GHG Protocol Technical
Guidance PDF
- Subset of metrics (from
total in the standard)
deemed material to the
undertaking

Consider Scope 3 of Scope
3 from the outset and is it
in or out (not mandated
by PCAF not GHG
Protocol)

Internal Data Sourcing

ERP, procurement, travel,
asset registers (physical,
financial etc), HR, asset
management and
investment systems

- ERP: supplier spend,
product categories, POs
- Procurement master
data
- HR: headcount, FTE by
region
- Asset register: owned &
leased assets

Raw activity datasets per
category

Extract relevant activity
data for each category
from core systems

- SAP/Oracle ERP exports
- Concur travel logs
- HR (Workday) data

External Data Sourcing

Supplier ESG Portals,
Industry Databases,
emission factors (Efs)

- Supplier questionnaires
- CDP supply chain data
- Ecoinvent, DEFRA and
similar emission factors
- Industry benchmarks
- Scope 3 disclosures

External datasets & EF
library

API/pull data from ESG
databases, merge with
internal for completeness

- CDP API feed (this is now
licenced)
- DEFRA EF CSV
- EcoInvent datasets

Scope 3 as Part of ESRS Regulatory Reporting Workflow 3 of 6
Level 0: Data Preparation and Validation
Level 1

Level 2

Data Cleaning &
Harmonisation

Standardise units,
currencies, time

Inputs

- All raw data from
acquisition stage

Outputs

Data & Business Logic

Illustrative Data Feeds

Harmonised dataset with
unified schema

Unit conversion (kg to
tonnes, miles to km, $ to
€), temporal alignment to
reporting year

- ISO unit mapping table
- Internal vs external asset
class and instrument look
up tables

Automated completeness
check; % coverage by
spend, category,
geography, per and post
the remediation

- Missing data log

Can use common data
models such as that
available in MSFT Cloud
for Sustainability
Data Gap Identification
and Remediation

Missing category/activity
data checks

- Harmonised dataset

Gap report (missing data
flagged), missing data
imputed and estimated

Audit trail of imported
and estimated values

Statistical and ML riven
approach to data
estimation and imputation
(based on correlations,
distributions etc)
Data Validation

Automated & manual
checks

- Gap report + harmonised
data

Validated dataset

Threshold checks (e.g.,
year-on-year change >
±20%), reconciliation with
finance data

- Historical emissions
dataset
- Finance ledger
summaries

Scope 3 as Part of ESRS Regulatory Reporting Workflow 4 of 6
Level 0: Emission Calculation
Level 1

Level 2

Inputs

Outputs

Data & Business Logic

Illustrative Data Feeds

Apply Emission Factors

Map activities to EF

- Validated activity data
- EF library
- PCAF logic for Scope 3
Category 15

Emissions by activity line
item

Select EF by activity type,
region, material; apply
formula: Emissions =
Activity × EF

- DEFRA/Ecoinvent EF
table

Category Aggregation

Aggregate to ESRS
categories

- Emissions by activity
- PCAF logic for Scope 3
Category 15

Emissions by GHG
Protocol category & ESRS
E1

Sum emissions by
category, reconcile totals,
separate CO₂, CH₄, N₂O,
other gases

- Aggregation ruleset

Scope 3 as Part of ESRS Regulatory Reporting Workflow 5 of 6
Level 0: Assurance and Audit Trail
Level 1

Level 2

Inputs

Outputs

Data & Business Logic

Illustrative Data Feeds

Internal Review

Cross-check with business
units

- Category totals
- Source data extracts

Approved dataset

Business unit sign-off
workflow

- Review checklists

External Assurance
Support

Prepare evidence pack

- Approved dataset
- Data lineage
documentation
- Limited and reasonable
assurance standards
(existing SAE3000,
emerging ISSA500 etCV)

Assurance-ready pack

Provide traceability for
each figure to source data
and EF

- Audit log
- Source data extracts

Scope 3 as Part of ESRS Regulatory Reporting Workflow 6 of 6
Level 0: Reporting & Publication
Level 1

Level 2

Inputs

Outputs

Data & Business Logic

Illustrative Data Feeds

Disclosure Preparation

Format for ESRS

- Approved emissions data

ESRS-compliant disclosure
tables

Map emissions data to
ESRS E1-6 templates and
similar

- ESRS E1-6 guidance

Integration into Annual
Report / Sustainability
Report

Tagging for ESEF

- ESRS disclosures
- Narrative commentary

XHTML + XBRL tagged
disclosures

Apply ESEF taxonomy,
validate with filing tools
and submit / publish

- ESEF taxonomy XML

L&G Scope 3 Portfolio Analysis –
Detailed Process Specification

Data

Investment

Climate and nature

Reporting

Scope 3 Portfolio Analysis (Light Green Only)
End to End Workflow Perspective
Scope 3 Data Acquisition

Emissions Calculation

•
•
•

•
•

Identify Relevant Categories
Internal Data Sourcing
External Data Sourcing

Apply Emission Factors
Category Aggregation

Reporting & Publication
•
•

Scope 3
Data
Acquisition

Emissions
Calculation
Data
Preparation
&
Validation

Data Preparation & Validation
•
•
•

Data Cleaning & Harmonisation
Data Gap Identification
Data Validation

Disclosure Preparation
Integration into Annual Report /
Sustainability Report

Reporting &
Publication
Assurance
& Audit
Trail

Data Preparation & Validation
•
•

Internal Review
External Assurance Support

Agentic AI Functional Design
Data Acquisition Agents: Handle
ERP/API integrations, interface to
supplier portals etc
Data Validation Agents: Run
harmonisation, QA, anomaly
detection
Emission Calculation Agents: Map to
Emission Factors (EFs), perform
category aggregation

Assurance Agents: Compile
evidence packs, track lineage
Reporting Agents: Format, tag, and
publish disclosure

Agentic AI Functional Design
Prompt: Calculate my Scope 3 Category 15 (financed emissions) position for the following set of
investments consisting of bonds, infrastructure equity and real estate equity. As part of this analysis, I will
need to understand my ‘Scope 3 of my Scope 3’ so the emissions within the supply chain of my investments
Use action: User is allowed up upload the portfolio investment spreadsheet. The spreadsheet contains the
list of listing of all the bonds being invested in, the details of the infrastructure being invested in (e.g.
renewable energy projects, transport infrastructure, etc.) and the real estate being invested in
DAG architecture
• Data Acquisition Agents: Handle ERP/API integrations, interface to supplier portals etc
• Data Validation Agents: Run harmonisation, QA, anomaly detection
• Calculation Agents: Map to Emission Factors (EFs), perform category aggregation
• Assurance Agents: Compile evidence packs, track lineage
• Reporting Agents: Format, tag, and publish disclosure

1. Data acquisition agents
Agents will need to get three types of data: Internal data, external data, and public data

Sub-process 1.1: Collect internal portfolio data and guidance
•

Inputs
•

•

•

•

Processing logic
•

•

Internal data sources - Excel input file provided by the end user: This includes bond holdings, real estate equity holdings, and infrastructure equity holdings, along
with metadata for each investment: Issuer or asset name, internal and external unique identifiers (e.g. ISIN, property ID), i nvestment value or outstanding amount,
ownership stake, and any internal classification (sector, geography etc), location (for physical assets), known properties of the assets (e.g. utilisation, size, green design
ratings etc), website of the issuers and/or operators of the investment vehicle
Internal data sources - Climate Solution team: Obtain internal guidance from the Climate Solution team - PDF file with a guidance on how to categorize or flag certain
holdings (e.g. identify if a bond is green or a project-specific instrument) as part of context
Internal data sources - Middle Office and Reporting team (Institutional Retirement, Asset Management): Pull in any available internal data on investee financials (for
example, if L&G has internal analysis or prior data on a company’s revenue or enterprise value)
Consolidate all holdings into a unified master holdings dataset. Ensure that each investment is labelled with its type (bond, real estate equity, infrastructure equity),
and link any relevant internal identifiers

Outputs
•

A master holdings dataset, listing every in-scope investment with its key attributes and financial data. This will be the foundational dataset (as a database) for further
enrichment. For example, a table with columns: Investment ID, Name, Asset Class (bond/real estate/infra), Amount Invested (£), % Ownership (if applicable), Sector,
etc. This output is a database for ease of merging with emissions data

1. Data acquisition agents
Sub-process 1.2: Obtain carbon & financial data
•

Inputs
•

•

•

Internal data sources - Climate Solution team: Assume they manage relationships with carbon data vendors and maintain internal databases of climate data. Obtain
•
Latest GHG emissions data for companies and assets
•
Any mapping files that link L&G internal investment list to external data (e.g. matching company names or IDs) as part of ven dor data/mapping responsibilities
•
Proxy methodologies documentation (detailing how to estimate emissions when data is missing) and any climate scenario data relevant for later analysis.
External data sources
•
Reported emission data (primary - GHG emissions data for investees: For bond issuers, use third-party data from CDP disclosures, ISS ESG Rating and Intelligence
emissions data, regulatory filings. Look up both Scope 1 and 2 data as that is mandatory per PCAF as well as Scope 3 where disclosed
•
Supply chain data (primary) - corporate websites, regulatory filings: Crawl the websites to decompose company supply chain, assets, plants and materials
•
Emission estimation data (secondary): For real estate assets (properties, infrastructure projects), we need data like energy consumption or sector emissions intens ity
averages. Obtain PCAF emission factor database, US and UK government websites (property, conversion data, transportation)
•
Company financial metrics: To calculate attribution (what share of a company’s emissions L&G finance), we need metrics like Enterprise Value Including C ash (EVIC)
for companies (the sum of market capitalization and debt, per PCAF standards), or total project value for projects. Use finan cial data providers (Bloomberg, ISS) or
from the investees’ financial reports. For real estate assets, external valuations or property values might be required if not in internal records - assume we get this
from an external service provided by Real Estate and Infrastructure Valuation Agent
•
Industry benchmarks, national and emission factors: Obtain datasets for emission intensity benchmarks (e.g. average CO₂ per unit revenue or per square foot for
sectors, national average building emissions, energy benchmarks, sector intensity values, regional grids carbon intensity for power consumption, etc.) from sources
like the International Energy Agency (IEA), academic studies, or government databases. Get emission factor libraries (e.g. UK BEIS, DEFRA carbon conversion factors for
energy, if calculating emissions from energy use) to compute emissions for properties if only energy data is available

Processing logic
• Using the master holdings list from 1.1, map each investment to its corresponding external data. For each bond, match the issuer to the external emissions dataset (via a unique
identifier like an ISIN-to-company mapping, or company name matching). Leverage The Climate Solutions team’s mapping files or vendor tools here by using their crossreference (ensuring that, e.g. a bond issued by Company X is linked to Company X’s emissions data)
• For real estate or infrastructure assets, determine if there is direct emissions data available (e.g. from an asset manager or operator); if not, plan and tag to use proxy data later
• Gather all matched emissions figures and required financial metrics (EVIC, revenues, etc.) for the investees

•

Outputs
Based on the master holdings list from three key datasets emerge:
•
Emissions dataset - a list of companies/projects with their GHG emissions (tCO₂e) and possibly other climate metrics
•
Reference dataset – financial parameters for those companies/projects (EVIC, etc.), and any needed benchmark factors
•
A data mapping log documenting which investments were successfully matched to external data, and which were not (flagging unmapp ed items for follow-up in validation

1. Data acquisition agents
Sub-process 1.3: Integrate and initial merge
• Inputs
• Internal data sources: The outputs from sub-process 1.1 (portfolio holdings) and 1.2 (emissions and reference data) -both are needed her
• External data sources: No new external input in this step, beyond what was gathered in 1.2

• Processing logic
• Merge the internal and external datasets to create a unified view per investment: join the holdings table with the emissions data and financial metrics using the mapping
established. After merging, each investment should have: its financial attributes (investment value, etc.), the investee’s emissions (Scope 1 and 2), and the investee’s EVIC
or project value
• Scope 3 of Scope 3 should be held separate and estimated using the inferred Scope 3 activities
• For any investments that could not be matched to external data (e.g., a private company with no disclosed footprint, or a small project without data), leave placeholders
or default values (to be handled in validation). Ensure that the data types are consistent (e.g., emissions all in the same unit, currency consistency for financial data).

• Outputs
• A consolidated portfolio carbon dataset – effectively a table where each row is an investment and columns include all fields needed for emissions calculation (such as:
Investment ID, Name, Asset Class, Amount Invested, % Ownership, Company/Asset Emissions (tCO₂e), Company EVIC (£), etc.). Also, an initial data coverage report can
be produced, summarising how many investments have full data and highlighting data gaps (for example, “85% of portfolio by value has direct emissions data; 15% will
need estimation”). This sets the stage for the validation phase.
Example: After Phase 1, we will have a row like: Bond XYZ – £10M invested – Issuer: ABC Corp – Sector: Utilities – ABC Corp Emissions: 5,000 tCO₂e (Scope 1+2) and 25,000 tCO₂e
(Scope3) – ABC Corp EVIC: £50B
A row for a real estate asset will show Property 123 – £200M (75% ownership) - Building emissions: (not yet available, flagged for proxy)

2. Data validation agents
Sub-process 2.1: Data completeness and accuracy audit
• Inputs
•
•
•
•
•

Internal data sources - The consolidated dataset from Process 1
Internal data sources - Middle Office Team DB: Financial figures for a cross-check that the sum of all bond positions’ values matches known totals etc
Internal data sources - Reporting Team DB: Leverage last year’s disclosed data or baseline for comparison
Internal data sources - Climate Solutions Team DB: An internal database of known issues or last-known emissions for certain entities
External data sources - CDP, Bloomberg, Refinitiv etc: Alternative or secondary data sources for verification. i.e. if primary emissions data came from one vendor, we
want to spot-check a few large issuers against another source (like checking a company’s annual report or CDP report for the rep orted emissions). Also, the agent will use
publicly available data for sanity checks (e.g., if a company’s revenue is known, is the emissions figure roughly plausible for that industry?)

Processing Logic: Perform a systematic QA sweep:
• Identify any missing data fields: Any investment with blank emissions or missing EVIC or other needed input is catalogued (for example, “Property 123 – missing
emissions data”)
• Check for outliers or anomalies: Extremely high or low emission intensities (tCO₂e / £ invested) compared to peers. If a bond position in a small company shows an
unusually large emission number, flag it for review - it could indicate a data error or a mismatch
• Ensure consistency: If the same company appears multiple times (perhaps in bonds), verify that the emissions and EVIC data are identical for each instance (avoid
duplication or divergence in data for the same entity). Also verify the currency and units alignment (no mixing of CO₂ scopes or units).
• Compare aggregated figures with expectations: Compute a preliminary sum of emissions using available data to see if it’s in a reasonable range given known industry
benchmarks.
• Document findings in a data quality report: Lists issues like missing values, anomalies, and preliminary data quality score. (Using PCAF’s scoring scheme, data from
company disclosures = high quality, proxies = lower quality etc)

Outputs:
• A list of data issues that need resolving (e.g. 10 investments with missing emissions, 2 outliers to double-check etc)
• An initial data quality assessment for each data point (possibly tagging each investment’s emissions data with PCAF quality grade 1-5)
• Also, an updated dataset where obvious errors have been corrected (if any minor fixes can be done immediately, like correcting a unit error). This sub-process sets the
stage for targeted proxy data application and final confirmation

2. Data validation agents
Sub-process 2.2: Apply proxy data & assumptions
Inputs
• Internal data source - Climate Solutions: Guidance documents or tools from the Climate Solutions team on how to handle missing data. This may include an
internal ’Proxy Methodology Paper’ that outlines accepted approaches (for example: “if a company’s emissions are missing, use the sector average emissions per revenue
multiplied by the company’s revenue). Also, the team itself, which can generate proxy values using their models
• Internal data source - Group Climate: Obtain have preferences based on regulatory expectations - e.g., to use conservative estimates or to disclose certain assumptions
explicitly
• External data sources - IEA, industry-average emission factors: Other secondary data identified in Phase 1 (energy benchmarks, sector intensity values, regional grids
carbon intensity for power consumption, etc.). For example, if a property’s actual energy usage is unknown, use average energy usage per square meter for that building
type and local grid emission factor to estimate its emissions. If a company’s emissions are missing, use an emissions-to-revenue ratio from a similar peer or sector
average. Also, external databases like IEA or research papers for sector intensities serve as inputs.

Processing logic: For each data gap identified in 2.1, fill in a reasonable estimate:
• If emissions data is missing for a corporate investment, calculate a proxy e.g. multiply the company’s revenue by the average emissions per revenue for that industry and
region (or use known emissions of a comparable company). If the company is known in size, scale by that. This yields an estimated Scope 1+2 emission figure
• Real estate: If a real estate asset lacks measured emissions, estimate its emissions from building characteristics: use the size (square footage) * typical energy intensity
(kWh/m²) * carbon factor (kgCO₂e/kWh). Adjust for occupancy or efficiency class if known (for instance, if it’s known as a green-certified building, adjust downward)
• Document each assumption clearly: e.g. ‘Company X - no reported data, used sector average intensity of 0.5 tCO₂e/£m revenue from Source Y’ or ‘Property 123 assumed energy usage of 200 kWh/m² based on UK average office benchmarks’. Apply a conservative approach to avoid underestimation (as recommended for early
disclosures with data gaps) Mark these entries so that they can later be reported as estimated (this ties into transparency for TCFD reporting).

Outputs:
• An augmented portfolio dataset where all previously missing fields are now populated with either actual data or proxy estimates. The dataset now should be complete –
every investment has an emissions value and other required inputs
• Alongside this, produce an Assumptions Register detailing each proxy and assumption used. This register will be important for assurance and disclosure (showing where
data is less certain). We also update the data quality scores: proxies would typically be categorized as lower quality (PCAF score 4 or 5) compared to reported data (score
1-3)
• The output at this stage is effectively a finalised input dataset for calculation, with full coverage and noted quality flags.

2. Data validation agents
Sub-process 2.3: Data verification and sign-off (asynchronous ‘human in the loop’ with an options for interim / immediate
verification agents)
Inputs
• Internal data sources: The now-complete dataset and the assumptions register. Also, internal stakeholders are 1) Middle Office (verifies that investment values and any
financial figures align with official records (no inadvertent changes during processing); 2) Climate Solutions team double-checks that proxies were applied correctly; 3)
Group Climate team reviews the dataset against reporting requirements (ensuring, for instance, that any use of estimated data is acceptable & will be disclosed properly)
• External data sources - issuer and operator engagement portal: Share parts of the data back with investees or use counterparty engagement to validate critical items.
For example, where a proxy was used for a major holding’s emissions, reaching out to that company’s sustainability team via the Portal to confirm if more recent data is
available
• External data sources - benchmark comparison: Alternatively, compare a sample of our results with peers or industry reports (if another asset manager published a
footprint for a similar asset, do our figures seem reasonable)

Processing Logic: Asynchronous - convene an internal review meeting or process where each key team signs off on the data.
Asynchronous - agents perform the validation
• Climate Solutions Team / Agent: Validates that the technical aspects (emissions data and proxies) are handled correctly and aligns with methodologies. They e nsure the
dataset is consistent with PCAF standards and that data quality scores are assigned appropriately.
• Middle Office Team / Agent: Confirm that for each investment, the financial attributes (amounts, ownership percentages) match the official books. Any dis crepancies
(perhaps due to valuation dates, etc.) are reconciled.
• Institutional Retirement Reporting Team / Agent: Verifies that the dataset scope matches the intended reporting scope (e.g., if certain portfolios were meant to be
included/excluded) and that the outputs can roll up to the level needed for disclosure (for instance, if they will report at an aggregate institutional level, ensure all
relevant assets are included exactly once).
• Group Climate Team / Agent: Checks the dataset against regulatory expectations: for instance, if the TCFD report requires year-on-year changes, do we have last year’s
data to compare; if required to disclose data quality or methodologies, is the info recorded. They might also ensure that scenario analysis data (outside this dataset)
remains consistent (though scenario analysis is separate, no conflicting numbers).
• Address any feedback: e.g., if a proxy seems too high-level, perhaps refine it; if an asset should be classified differently, adjust that classification.

Outputs:
• Validated and approved dataset (final input for calculations). At this point, we have effectively a green-light to proceed to emission calculations, with confidence in the
data. All investments have emissions data (actual or estimated) and all inputs are vetted
• The finalized assumptions log and a data quality summary. This could include metrics like “% of portfolio emissions based on reported data vs estimated” which might be
disclosed or at least noted internally. The sign-off from relevant teams is recorded (for governance purposes, showing that multiple lines of defence have reviewed the
data). By the end of Phase 2, the risk of errors is minimized

3. Emission calculation

Sub-process 3.1: Calculate Emissions for Corporate Bonds
Inputs
• Internal data sources: inputs come directly from the validated dataset of Phase 2

Processing Logic:
• For corporate bonds and listed equity holdings we have: the investment value (e.g. amount of bond holding), the enterprise value (EVIC) of the issuer, and the issuer’s GHG
emissions (Scope 1 and 2). These inputs come directly from the validated dataset of Phase 2
• Method & Processing Logic: We apply the PCAF attributed emissions formula. For each corporate issuer::

• For example, if we own 1% of a company (by value), we assume responsibility for 1% of that company’s emissions
• We ensure to use the same emissions figure for the company whether we hold equity or debt, to avoid double counting. If the firm has multiple securities in the portfolio (e.g.,
we hold both bond and equity of Company X), we calculate emissions for each holding and then sum them – effectively still getting (bond value + equity value)/EV * emissions,
which is total exposure fraction times emissions (same result as if done once combined). We need to be careful not to count the company twice in aggregate results.
• If any company emissions include Scope 3 (not typical unless explicitly intended), we would handle them separately to avoid mixing scopes. In general, we stick to Scope 1 and 2
for the financed emissions footprint as per common practice. However, in this case L&G wants to see Scope 3 in the final output
• For sovereign bonds (not explicitly mentioned, but if any government bonds were present, PCAF has a different method. For this mock up
• The calculation is performed programmatically for all relevant rows. After computing each, we sum up emissions by issuer, by sector, etc., as needed.
• Outputs: Financed emissions for each corporate issuer investment, typically expressed in tons CO₂e.
• We can aggregate these at various levels for reporting: e.g., total financed emissions from corporate bonds, total from listed equities (if any), and combined. We also compute
an emissions intensity metric for this segment, such as tons CO₂e per £ million invested, to facilitate comparisons and to meet any disclosure requirement for intensity. For
instance, if our total financed emissions in this category is 50,000 tCO₂e on £5,000M invested, the intensity is 10 tCO₂e/£m. Another metric is Weighted Average Carbon
Intensity (WACI), but that’s a different calculation (often required by TCFD, but WACI uses emissions/ revenue weighted by portfolio weight – which could be computed
separately if needed). The primary output is absolute emissions attributable to our financing of corporates, in a tabular form ready to aggregate with other asset classes. We
note the portion of these emissions that were based on estimated data (from the data quality info, e.g., “5% of this total is derived from proxy data”), for transparency1

3. Emission calculation

Sub-process 3.2: Calculate Emissions for Real Estate Equity
Inputs
• Internal data sources: inputs come directly from the validated dataset of Phase 2

Processing Logic:
• For each property asset, we have either actual operational emissions data or an estimated emissions figure (from Phase 2, likely calculated via energy use or proxy). We also
have our ownership share in the asset (often 100% if wholly owned, or some percentage if co-owned). In lieu of EVIC (since a building isn’t a company with debt/equity split in
the same way), the attribution factor is the ownership percentage or financed fraction of the asset’s value.
• Method & Processing Logic: Formally, for each property p:

• The Property Emissions would ideally be the sum of Scope 1 and 2 emissions from building operations (e.g., fuel combustion on-site, purchased electricity) over a year. We
ensure those emissions are either measured (from utility data, fuel usage) or estimated via the proxies applied earlier. If properties are held via a property company or fund
structure, but essentially we have look-through to the asset, we use the same approach.
• For each property, if we had to compute emissions via energy, we do: Energy use (kWh) × carbon factor (kgCO₂e/kWh) for each energy type, summing to total tCO₂e3. (This
would have been done in data acquisition or validation step for proxies; here we just use the result).
• No additional financial factor like EVIC is needed since we’re directly allocating by ownership. The “investment value” in a sense is proportional to ownership anyway. If needed,
we could also check consistency by taking (our investment value / property value) as the fraction – it should equal our ownership share. If not, that implies perhaps leverage on
the property; however, since we are equity owners, PCAF would treat it as if we finance that share of emissions fully.
• We aggregate emissions across all real estate assets

• Outputs: Financed emissions for each real estate investment, typically expressed in tons CO₂e.
• Financed emissions from real estate portfolio (tCO₂e). This could be broken down by property or summed. Likely we’ll report an aggregate figure for real estate investments.
Additional outputs might include the breakdown by type of emissions (e.g., electricity vs gas usage contributions) if needed internally, and an intensity metric like kg CO₂e per
square meter across the portfolio to align with any real estate benchmarks
• for external disclosure, usually the total financed emissions attributed to real estate holdings is the key number. We again record the PCAF data quality (perhaps many property
emissions might be estimated if direct data was unavailable). If any properties were only partially owned, that is already accounted in the calculation by the share.

3. Emission calculation

Sub-process 3.3: Calculate Emissions for Infrastructure Equity
Inputs
• Internal data sources: inputs come directly from the validated dataset of Phase 2

Processing Logic:
• For each project, we should have an emissions figure (either reported by the project operator or estimated via proxies, depending on data availability from Phase 2) and our
ownership percentage or share of financing. If the infrastructure investment is through a fund, we may have to look through to underlying projects proportionally.
• Method & Processing Logic: PCAF’s project finance methodology is similar to bonds : allocate based on share of total project cost financed. For each project j:

• This mirrors the approach for real estate, using share of equity or loan in the project as attribution. If we provided, say, 20% of the capital for an infrastructure project, we take
responsibility for 20% of its annual emissions.
• Project Emissions should cover operational emissions of the infrastructure. For example, if it’s a renewable energy project, operational emissions might be low (mostly
maintenance vehicles, etc.), whereas a transportation infrastructure (like a toll road) might have more significant operational footprint (energy use for lighting, etc.). If it’s a
power generation project (e.g., a gas power plant), the combustion emissions would be the major contributor. We use whatever data is available; if none, we use industry proxy
(like emissions per MW for similar plants, etc.).
• Ensure that any debt vs equity distinction in project finance is considered: if our investment is equity and the project also has loans, PCAF would still have us count our portion
of emissions based on total project value financed. In practice, since we’re likely calculating for “infrastructure equity”, we assume equity share as the proportion. (If there were
project loans we provided, it would be similar calculation treating loan as part of financing).
• Compute for each project and sum up.

• Outputs: Financed emissions for each infrastructure equity investment (project finance), typically expressed in tons CO₂e
• inanced emissions from infrastructure investments (tCO₂e). Again, aggregated as needed (total for this category). We might also note project-level results internally, especially if
we want to identify high emission projects vs low (e.g., a list: Project A – 1,000 tCO₂e, Project B – 0 tCO₂e if it’s a wind farm, etc.). This could feed into future strategy (which
projects to decarbonise)
• For reporting, likely the total figure is disclosed, possibly with qualitative commentary on the nature of those assets. Intensity metrics here are less standard, but one could
compute something like tCO₂e per £m invested in infrastructure for internal comparison.

Scope 3 insight and decision
making

Scope 3 as Part of Sustainable Investment, Climate Risk Management and
Portfolio Management Decision Making - Candidates (Illustrative)
Hypothesis (testable statement)

Key Data Inputs

Modelling / Analytics Required

Climate adjusted financial metrics (e.g. emissionsadjusted yield curves): Using corporate bonds asasn
example, issuers of such bonds with high Scope 3
intensity will see steeper yield curves due to anticipated
transition risk premia.

Bond yield curves by issuer
- Scope 3 intensity (tCO₂e / $ revenue)
- Sector decarbonisation pathways
- Carbon price forecasts

- Regression of yield spreads vs Scope 3 intensity
controlling for credit risk
- Scenario analysis on carbon pricing impacts on
refinancing costs
- Build adjusted yield curves with risk premia

Portfolio Value-at-Risk (VaR) uplift from Scope 3
transition scenarios: Integrating Scope 3 transition
shocks increases portfolio VaR, leading to allocation
shifts.

- Portfolio holdings
- Issuer Scope 1–3 data
- Transition risk scenarios (NGFS)
- Sector elasticity to carbon prices

- Climate stress testing (e.g., PACTA, MSCI CVaR) with
Scope 3 included
- Monte Carlo simulation of carbon price shocks
- Sensitivity analysis of VaR by sector

Physical risk correlation with Scope 3 hotspots: Assets
in sectors with high Scope 3 also face higher physical risk
to operations/value chains, raising default risk.

- Physical risk maps (flood, heat, drought)
- Supplier geolocation data
- Scope 3 category breakdown

- Overlay Scope 3 upstream geography with climate
hazard layers
- Bayesian network linking supply disruption to credit risk
- Correlation analysis between hazard exposure and
Scope 3 intensity

Sector rotation strategy based on Scope 3 abatement
cost curves: Shifting portfolio weight to sectors where
marginal abatement cost (MAC) of Scope 3 is lowest
generates outperformance in tightening policy
environments.

- MAC curves by sector
- Scope 3 baselines
- Policy tightening probability

- Optimisation: maximise expected return subject to
carbon budget
- Game-theoretic modelling of sector decarbonisation

Climate adjusted financial metrics - from George to build and layer
Cash Flows – Top Down
Traditional Climate Risk Measurement
is a top-down exercise

Issuer

Measure Type

Yield

Total

2025

2026

2027

2028

2029

2030

2031

2032

2033

2034

2035

2036

2037

2038

2039

2040

Traditional
Climate RCP 2.6
Climate RCP 4.5
Climate RCP 8.0

AI enables granular estimates of climate and
transition hazards by geography and asset –
even if the data is not readily available
Leverage AI to generate
granular inventory of
issuer assets and revenue
generating entities

Assets
Offices

Map assets to
location specific
physical and
transition risks

Aggregate to create firm
level view of climate risk

Physical Risks
Asset Name

Transition Risks
Litigation
Energy Costs

Plants
Policy
Supply Chain

Technology

Raw Materials

Reputation
Leverage AI Data
pipelines to ingest
climate data and
damage functions

Cash Flows – Bottom Up

Climate Models & Damage
Functions

Measure Type

Office

Climate RCP 2.6

Plant 1

Climate RCP 2.6

Plant 2

Climate RCP 2.6

Raw Material A

Climate RCP 2.6

Office

Climate RCP 4.5

Plant 1

Climate RCP 4.5

Plant 2

Climate RCP 4.5

Raw Material A

Climate RCP 4.5

Office

Climate RCP 8.0

Plant 1

Climate RCP 8.0

Plant 2

Climate RCP 8.0

Raw Material A

Climate RCP 8.0

Yield

Total

2025

2026

2027

2028

2029

2030

2031

2032

2033

2034

Gareth’s Team
Modelling
Calculations

Data From
IM
Validation

Data sent to group

Directly Sourced
Data

Transformation

Orchestration

Enabling additional insights
Having highly paid/skilled people
working value add

Enables better insight from you data
26

