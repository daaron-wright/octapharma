L&G ESG Scope 3
Workflow and agentic AI
functional architecture
July 2025

Extract from 8 August
session
th

Data

Investment

Climate and nature

Reporting

Business need exploration with L&G

Leveraging Kyndryl’s Agentic AI Framework (a self organising, dynamic system) to address individual use
cases across the Institutional Retirement pricing, climate risk and nature modelling, and sustainable
investment
Illustrative and not
Agentic AI as integration
layer(‘glue’) across multiple
fragmented systems,
removing the need for data
centralization

Going beyond the basics on
addressing the data quality
issues, estimations and
imputation of missing values

Federated
Data
Collection
and
Aggregation

Curation of internal and
external data into unique
knowledge & data assets
(e.g. for peer benchmarking,
pricing, portfolio risk
management etc)

AI-driven economic model definition,
orchestration, specification and simulation
on how shifts in technology, policy,
reputation, market and customer behavior
preferences impact companies, especially
based on their emissions profiles

Portfolio
Optimization
with Emission
Constraints

Data and
Knowledge
Asset
Curation

HighResolution
Physical Risk
Modeling

Dynamic
Pricing &
Yield
Calculation

Anomaly
Detection

Timely detection of outliers
and anomalies in the raw
data feeds (internal and
external), portfolio level data
for greater trust, better risk
management and early
identification of

Using AI on satellite data,
sensor feeds, climate model
outputs to identify patterns and
provide high-resolution risk
maps for assets

Dynamic
Transition
Risk
Simulation
AI Powered
Data Quality
Improvement

AI optimization algorithms to
recommend portfolio
adjustments that meet return
objectives while minimising
financed emissions
(optimization under
constraint)

exhaustive, to be
explored based on Legal &
General priorities, pain
points and opportunities

AI - Driven
Reporting
and
Compliance

Efficient and effective
compliance with Scope 3
regulations, including horizon
scanning of regulatory
evolution

Agentic AI determination of climate factors
affect asset valuation (e.g. learn correlation
between market data and Scope 3 exposure to
better understand volatility, yields, credit risk
and price in appropriately)

High certainty and can be mocked up
with relative ease

Data collection and
regulatory reporting
workflow (end to end)

Data

Investment

Climate and nature

Scope 3 as Part of ESRS Regulatory Reporting Workflow 1 of 6
End to End Workflow Perspective
Scope 3 Data
Acquisition
•
•
•

Identify Relevant
Categories
Internal Data Sourcing
External Data Sourcing
Scope 3
Data
Acquisitio
n

Data Preparation &
Validation
•
•
•

Emissions Calculation
•
•

Apply Emission Factors
Category Aggregation

Reporting & Publication
•
•

Data
Preparatio
n&
Validation

Data Cleaning &
Harmonisation
Data Gap Identification
Data Validation

Emissions
Calculatio
n

Assurance &
Audit Trail

Disclosure Preparation
Integration into Annual Report
/ Sustainability Report

Reporting
&
Publicatio
n

Data Preparation &
Validation
•
•

Internal Review
External Assurance Support

Reporting

Agentic AI Functional
Design
Data Acquisition Agents:
Handle ERP/API integrations,
interface to supplier portals etc
Data Validation Agents: Run
harmonisation, QA, anomaly
detection
Emission Calculation Agents:
Map to Emission Factors (EFs),
perform category aggregation
Assurance Agents: Compile
evidence packs, track lineage
Reporting Agents: Format, tag,
and publish disclosure

Scope 3 as Part of ESRS Regulatory Reporting Workflow 2 of 6
Level 0: Scope 3 Data Acquisition
Level 1

Identify Relevant
Categories

Level 2

Inputs

Outputs

Data & Business Logic

Illustrative Data Feeds

Category mapping to
ESRS GHG Protocol

- Double materiality
matrix (internal)
- Previous year’s Scope
3 mapping (internal)
- GHG Protocol
guidance (external)

List of applicable
Scope 3 categories
with ESRS (based on
IG3 list of metrics and
similar)

Map internal
materiality results to
GHG Protocol
categories, align with
ESRS E1 requirements

- Internal ESG
materiality output
- GHG Protocol
Technical Guidance
PDF
- Subset of metrics
(from total in the
standard) deemed
material to the
undertaking

- ERP: supplier spend,
product categories,
POs
- Procurement master
data
- HR: headcount, FTE
by region
- Asset register: owned
& leased assets
- Supplier
questionnaires
- CDP supply chain
data
- Ecoinvent, DEFRA and

Raw activity datasets
per category

Internal Data Sourcing

ERP, procurement,
travel, asset registers
(physical, financial etc),
HR, asset
management and
investment systems

External Data Sourcing

Supplier ESG Portals,
Industry Databases,
emission factors (Efs)

External datasets & EF
library

Consider Scope 3 of
Scope 3 from the
outset and is it in or
out (not mandated by
PCAF not GHG
Protocol)
Extract relevant
activity data for each
category from core
systems

API/pull data from ESG
databases, merge with
internal for
completeness

- SAP/Oracle ERP
exports
- Concur travel logs
- HR (Workday) data

- CDP API feed (this is
now licenced)
- DEFRA EF CSV
- EcoInvent datasets

Scope 3 as Part of ESRS Regulatory Reporting Workflow 3 of 6
Level 0: Data Preparation and Validation
Level 1

Data Cleaning &
Harmonisation

Data Gap
Identification and
Remediation

Level 2

Standardise units,
currencies, time

Missing
category/activity data
checks

Inputs

- All raw data from
acquisition stage

- Harmonised dataset

Outputs

Data & Business Logic

Illustrative Data Feeds

Harmonised dataset
with unified schema

Unit conversion (kg to
tonnes, miles to km, $
to €), temporal
alignment to reporting
year

- ISO unit mapping
table
- Internal vs external
asset class and
instrument look up
tables

Automated
completeness check;
% coverage by spend,
category, geography,
per and post the
remediation

- Missing data log

Can use common data
models such as that
available in MSFT
Cloud for
Sustainability
Gap report (missing
data flagged), missing
data imputed and
estimated
Audit trail of imported
and estimated values

Data Validation

Automated & manual
checks

- Gap report +
harmonised data

Validated dataset

Statistical and ML
riven approach to data
estimation and
imputation (based on
correlations,
Threshold checks
distributions
etc) (e.g.,
year-on-year change >
±20%), reconciliation
with finance data

- Historical emissions
dataset
- Finance ledger
summaries

Scope 3 as Part of ESRS Regulatory Reporting Workflow 4 of 6
Level 0: Emission Calculation
Level 1

Level 2

Inputs

Outputs

Data & Business Logic

Illustrative Data Feeds

Apply Emission Factors

Map activities to EF

- Validated activity
data
- EF library
- PCAF logic for Scope
3 Category 15

Emissions by activity
line item

Select EF by activity
type, region, material;
apply formula:
Emissions = Activity ×
EF

- DEFRA/Ecoinvent EF
table

Category Aggregation

Aggregate to ESRS
categories

- Emissions by activity
- PCAF logic for Scope
3 Category 15

Emissions by GHG
Protocol category &
ESRS E1

Sum emissions by
category, reconcile
totals, separate CO₂,
CH₄, N₂O, other gases

- Aggregation ruleset

Scope 3 as Part of ESRS Regulatory Reporting Workflow 5 of 6
Level 0: Assurance and Audit Trail
Level 1

Level 2

Inputs

Outputs

Data & Business Logic

Illustrative Data Feeds

Internal Review

Cross-check with
business units

- Category totals
- Source data extracts

Approved dataset

Business unit sign-off
workflow

- Review checklists

External Assurance
Support

Prepare evidence pack

- Approved dataset
- Data lineage
documentation
- Limited and
reasonable assurance
standards (existing
SAE3000, emerging
ISSA500 etCV)

Assurance-ready pack

Provide traceability for
each figure to source
data and EF

- Audit log
- Source data extracts

Scope 3 as Part of ESRS Regulatory Reporting Workflow 6 of 6
Level 0: Reporting & Publication
Level 1

Level 2

Inputs

Outputs

Data & Business Logic

Illustrative Data Feeds

Disclosure Preparation

Format for ESRS

- Approved emissions
data

ESRS-compliant
disclosure tables

Map emissions data to
ESRS E1-6 templates
and similar

- ESRS E1-6 guidance

Integration into
Annual Report /
Sustainability Report

Tagging for ESEF

- ESRS disclosures
- Narrative
commentary

XHTML + XBRL tagged
disclosures

Apply ESEF taxonomy,
validate with filing
tools and submit /
publish

- ESEF taxonomy XML

L&G Scope 3 Portfolio
Analysis – Detailed Process
Specification

Data

Scope 3 Portfolio Analysis (Light Green Only)
End to End Workflow Perspective
Scope 3 Data
Acquisition
•
•
•

Identify Relevant
Categories
Internal Data Sourcing
External Data Sourcing
Scope 3
Data
Acquisitio
n

Data Preparation &
Validation
•
•
•

Emissions Calculation
•
•

Apply Emission Factors
Category Aggregation

Reporting & Publication
•
•

Data
Preparatio
n&
Validation

Data Cleaning &
Harmonisation
Data Gap Identification
Data Validation

Emissions
Calculatio
n

Assurance
& Audit
Trail

Disclosure Preparation
Integration into Annual Report
/ Sustainability Report

Reporting
&
Publicatio
n

Assurance & Audit Trail
•
•

Internal Review
External Assurance Support

Investment

Climate and nature

Reporting

Agentic AI Functional
Design
Data Acquisition Agents:
Handle ERP/API integrations,
interface to supplier portals etc
Data Validation Agents: Run
harmonisation, QA, anomaly
detection
Emission Calculation Agents:
Map to Emission Factors (EFs),
perform category aggregation
Assurance Agents: Compile
evidence packs, track lineage
Reporting Agents: Format, tag,
and publish disclosure, not in
scope (beyond summary
dashboard)

Data

Scope 3 Portfolio Analysis
Scope 3 Portfolio Analysis Workflow
Perspective
Scope 3 Data
Acquisition
•
•
•

Identify Relevant
Categories
Internal Data Sourcing
External Data Sourcing
Scope 3
Data
Acquisitio
n

Data Preparation &
Validation
•
•
•

Emissions Calculation
•
•

Apply Emission Factors
Category Aggregation

Data
Preparatio
n&
Validation

Data Cleaning &
Harmonisation
Data Gap Identification
Data Validation

Emissions
Calculatio
n

Assurance
& Audit
Trail

Assurance & Audit Trail
•
•

Internal Review
External Assurance Support

Investment

Climate and nature

Agentic System Design
Data Acquisition Agents:
Handle internal and external
integrations, interface to data
suppliers etc
Data Validation Agents: Run
harmonisation, QA, anomaly
detection
Emission Calculation Agents:
Map to Emission Factors (EFs),
perform category aggregation
Assurance Agents: Compile
evidence packs, track lineage

Reporting

Agentic AI Functional Design
Prompt: Calculate my Scope 3 Category 15 (financed emissions) position for the following set
of investments consisting of bonds, infrastructure equity and real estate equity. As part of
this analysis, I will need to understand my ‘Scope 3 of my Scope 3’ so the emissions within
the supply chain of my investments
Use action: User is allowed up upload the portfolio investment spreadsheet. The
spreadsheet contains the list of listing of all the bonds being invested in, the details of the
infrastructure being invested in (e.g. renewable energy projects, transport infrastructure,
etc.) and the real estate being invested in
DAG architecture
• Data Acquisition Agents: Handle ERP/API integrations, interface to supplier portals etc
• Data Validation Agents: Run harmonisation, QA, anomaly detection
• Calculation Agents: Map to Emission Factors (EFs), perform category aggregation
• Assurance Agents: Compile evidence packs, track lineage
• Reporting Agents: Format, tag, and publish disclosure

1. Data acquisition agents

Agents will need to get three types of data: Internal data, external data, and public data

Sub-process 1.1: Collect internal portfolio data and guidance
•

Inputs
•

•
•

•

Processing logic
•

•

Internal data sources - Excel input file provided by the end user: This includes bond holdings, real estate equity holdings, and infrastructure
equity holdings, along with metadata for each investment: Issuer or asset name, internal and external unique identifiers (e.g. ISIN, property ID),
investment value or outstanding amount, ownership stake, and any internal classification (sector, geography etc), location (for physical assets),
known properties of the assets (e.g. utilisation, size, green design ratings etc), website of the issuers and/or operators of the investment vehicle
Internal data sources - Climate Solution team: Obtain internal guidance from the Climate Solution team - PDF file with a guidance on how to
categorize or flag certain holdings (e.g. identify if a bond is green or a project-specific instrument) as part of context
Internal data sources - Middle Office and Reporting team (Institutional Retirement, Asset Management): Pull in any available internal
data on investee financials (for example, if L&G has internal analysis or prior data on a company’s revenue or enterprise value)
Consolidate all holdings into a unified master holdings dataset. Ensure that each investment is labelled with its type (bond, real estate equity,
infrastructure equity), and link any relevant internal identifiers

Outputs
•

A master holdings dataset, listing every in-scope investment with its key attributes and financial data. This will be the foundational dataset (as a
database) for further enrichment. For example, a table with columns: Investment ID, Name, Asset Class (bond/real estate/infra), Amount
Invested (£), % Ownership (if applicable), Sector, etc. This output is a database for ease of merging with emissions data

1. Data acquisition agents

Sub-process 1.2: Obtain carbon & financial data
•

Inputs
•

•

•

Internal data sources - Climate Solution team: Assume they manage relationships with carbon data vendors and maintain internal databases of
climate data. Obtain
•
Latest GHG emissions data for companies and assets
•
Any mapping files that link L&G internal investment list to external data (e.g. matching company names or IDs) as part of vendor data/mapping
responsibilities
•
Proxy methodologies documentation (detailing how to estimate emissions when data is missing) and any climate scenario data relevant for
later analysis.
External data sources
•
Reported emission data (primary - GHG emissions data for investees: For bond issuers, use third-party data from CDP disclosures, ISS ESG
Rating and Intelligence emissions data, regulatory filings. Look up both Scope 1 and 2 data as that is mandatory per PCAF as well as Scope 3
where disclosed
•
Supply chain data (primary) - corporate websites, regulatory filings: Crawl the websites to decompose company supply chain, assets, plants
and materials
•
Emission estimation data (secondary): For real estate assets (properties, infrastructure projects), we need data like energy consumption or
sector emissions intensity averages. Obtain PCAF emission factor database, US and UK government websites (property, conversion data,
transportation)
•
Company financial metrics: To calculate attribution (what share of a company’s emissions L&G finance), we need metrics like Enterprise Value
Including Cash (EVIC) for companies (the sum of market capitalization and debt, per PCAF standards), or total project value for projects. Use
financial data providers (Bloomberg, ISS) or from the investees’ financial reports. For real estate assets, external valuations or property values
might be required if not in internal records - assume we get this from an external service provided by Real Estate and Infrastructure Valuation
Agent
•
Industry benchmarks, national and emission factors: Obtain datasets for emission intensity benchmarks (e.g. average CO₂ per unit revenue
or per square foot for sectors, national average building emissions, energy benchmarks, sector intensity values, regional grids carbon intensity
for power consumption, etc.) from sources like the International Energy Agency (IEA), academic studies, or government databases. Get emission
factor libraries (e.g. UK BEIS, DEFRA carbon conversion factors for energy, if calculating emissions from energy use) to compute emissions for
properties if only energy data is available

Processing logic

• Using the master holdings list from 1.1, map each investment to its corresponding external data. For each bond, match the issuer to the external
emissions dataset (via a unique identifier like an ISIN-to-company mapping, or company name matching). Leverage The Climate Solutions team’s
mapping files or vendor tools here by using their cross-reference (ensuring that, e.g. a bond issued by Company X is linked to Company X’s emissions

1. Data acquisition agents

Sub-process 1.3: Integrate and initial merge
• Inputs

• Internal data sources: The outputs from sub-process 1.1 (portfolio holdings) and 1.2 (emissions and reference data) -both are needed her
• External data sources: No new external input in this step, beyond what was gathered in 1.2

• Processing logic

• Merge the internal and external datasets to create a unified view per investment: join the holdings table with the emissions data and financial
metrics using the mapping established. After merging, each investment should have: its financial attributes (investment value, etc.), the investee’s
emissions (Scope 1 and 2), and the investee’s EVIC or project value
• Scope 3 of Scope 3 should be held separate and estimated using the inferred Scope 3 activities
• For any investments that could not be matched to external data (e.g., a private company with no disclosed footprint, or a small project without
data), leave placeholders or default values (to be handled in validation). Ensure that the data types are consistent (e.g., emissions all in the same
unit, currency consistency for financial data).

• Outputs

• A consolidated portfolio carbon dataset – effectively a table where each row is an investment and columns include all fields needed for
emissions calculation (such as: Investment ID, Name, Asset Class, Amount Invested, % Ownership, Company/Asset Emissions (tCO₂e), Company
EVIC (£), etc.). Also, an initial data coverage report can be produced, summarising how many investments have full data and highlighting data
gaps (for example, “85% of portfolio by value has direct emissions data; 15% will need estimation”). This sets the stage for the validation phase.

Example: After Phase 1, we will have a row like: Bond XYZ – £10M invested – Issuer: ABC Corp – Sector: Utilities – ABC Corp Emissions: 5,000 tCO₂e (Scope
1+2) and 25,000 tCO₂e (Scope3) – ABC Corp EVIC: £50B
A row for a real estate asset will show Property 123 – £200M (75% ownership) - Building emissions: (not yet available, flagged for proxy)

2. Data validation agents

Sub-process 2.1: Data completeness and accuracy audit
• Inputs

• 0. Internal data sources - The consolidated dataset from Process 1
1 ‘Get Internal Data for Validation, Imputation and Estimation’
• 1. Internal data sources - Middle Office Team DB: Financial figures for a cross-check that the sum of all bond positions’ values matches known
totals etc
• 1 Internal data sources - Reporting Team DB: Leverage last year’s disclosed data or baseline for comparison
• 1Internal data sources - Climate Solutions Team DB: An internal database of known issues or last-known emissions for certain entities
1 ‘Get External Data for Validation, Imputation and Estimation’
• 1. External data sources - CDP, Bloomberg, Refinitiv etc: Alternative or secondary data sources for verification. i.e. if primary emissions data
came from one vendor, we want to spot-check a few large issuers against another source (like checking a company’s annual report or CDP report
for the reported emissions). Also, the agent will use publicly available data for sanity checks (e.g., if a company’s revenue is known, is the emissions
figure roughly plausible for that industry?)

Processing Logic: Perform a systematic QA sweep:

• 2. ‘Identify Anomalies and Inconsistencies’
• Identify any missing data fields: Any investment with blank emissions or missing EVIC or other needed input is catalogued (for example,
“Property 123 – missing emissions data”)
• 2. Check for outliers or anomalies: Extremely high or low emission intensities (tCO₂e / £ invested) compared to peers. If a bond position in a
small company shows an unusually large emission number, flag it for review - it could indicate a data error or a mismatch
• 2. Ensure consistency: If the same company appears multiple times (perhaps in bonds), verify that the emissions and EVIC data are identical for
each instance (avoid duplication or divergence in data for the same entity). Also verify the currency and units alignment (no mixing of CO₂ scopes
or units).
• ‘3. ‘Compare against expectation’
• 3 Compare aggregated figures with expectations: Compute a preliminary sum of emissions using available data to see if it’s in a reasonable
range given known industry benchmarks.
• ‘Produce DQ report’
• 4 Document findings in a data quality report: Lists issues like missing values, anomalies, and preliminary data quality score. (Using PCAF’s
scoring scheme, data from company disclosures = high quality, proxies = lower quality etc)

Outputs:

2. Data validation agents

Sub-process 2.2: Apply proxy data & assumptions
Inputs

1 ‘Get Internal Data for Validation, Imputation and Estimation’
• 1. Internal data source - Climate Solutions: Guidance documents or tools from the Climate Solutions team on how to handle missing data. This
may include an internal ’Proxy Methodology Paper’ that outlines accepted approaches (for example: “if a company’s emissions are missing, use the
sector average emissions per revenue multiplied by the company’s revenue). Also, the team itself, which can generate proxy values using their
models
• 1. Internal data source - Group Climate: Obtain have preferences based on regulatory expectations - e.g., to use conservative estimates or to
disclose certain assumptions explicitly
• 1 ‘Get External Data for Validation, Imputation and Estimation’
• 1. External data sources - IEA, industry-average emission factors: Other secondary data identified in Phase 1 (energy benchmarks, sector
intensity values, regional grids carbon intensity for power consumption, etc.). For example, if a property’s actual energy usage is unknown, use
average energy usage per square meter for that building type and local grid emission factor to estimate its emissions. If a company’s emissions
are missing, use an emissions-to-revenue ratio from a similar peer or sector average. Also, external databases like IEA or research papers for
sector intensities serve as inputs.

Processing logic: For each data gap identified in 2.1, fill in a reasonable estimate:
• 5 ‘Estimate, Impute and Correct
• 5. If emissions data is missing for a corporate investment, calculate a proxy e.g. multiply the company’s revenue by the average emissions per
revenue for that industry 5. and region (or use known emissions of a comparable company). If the company is known in size, scale by that. This
yields an estimated Scope 1+2 emission figure
• 5. Real estate: If a real estate asset lacks measured emissions, estimate its emissions from building characteristics: use the size (square footage) *
typical energy intensity (kWh/m²) * carbon factor (kgCO₂e/kWh). Adjust for occupancy or efficiency class if known (for instance, if it’s known as a
green-certified building, adjust downward)
• 5. Document each assumption clearly: e.g. ‘Company X - no reported data, used sector average intensity of 0.5 tCO₂e/£m revenue from Source Y’ or
‘Property 123 - assumed energy usage of 200 kWh/m² based on UK average office benchmarks’. Apply a conservative approach to avoid
underestimation (as recommended for early disclosures with data gaps) Mark these entries so that they can later be reported as estimated (this
ties into transparency for TCFD reporting).

Outputs:

2. Data validation agents

Sub-process 2.3: Data verification and sign-off (asynchronous ‘human in the loop’ with an options for interim /
immediate verification agents)
Inputs

• Internal data sources: The now-complete dataset and the assumptions register. Also, internal stakeholders are 1) Middle Office (verifies that
investment values and any financial figures align with official records (no inadvertent changes during processing); 2) Climate Solutions team
double-checks that proxies were applied correctly; 3) Group Climate team reviews the dataset against reporting requirements (ensuring, for
instance, that any use of estimated data is acceptable & will be disclosed properly)
• Ignore for the demo for now
• External data sources - issuer and operator engagement portal: Share parts of the data back with investees or use counterparty
engagement to validate critical items. For example, where a proxy was used for a major holding’s emissions, reaching out to that company’s
sustainability team via the Portal to confirm if more recent data is available
• 1 ‘Get External Data for Validation, Imputation and Estimation’
• 1. External data sources - benchmark comparison: Alternatively, compare a sample of our results with peers or industry reports (if another asset
manager published a footprint for a similar asset, do our figures seem reasonable)

Processing Logic: Asynchronous - convene an internal review meeting or process where each key team signs off on
the data. Asynchronous - agents perform the validation

• 6. Review and Critic Agents
• 6. Climate Solutions Team / Agent: Validates that the technical aspects (emissions data and proxies) are handled correctly and aligns with
methodologies. They ensure the dataset is consistent with PCAF standards and that data quality scores are assigned appropriately.
• 6. Middle Office Team / Agent: Confirm that for each investment, the financial attributes (amounts, ownership percentages) match the official
books. Any discrepancies (perhaps due to valuation dates, etc.) are reconciled.
• 6. Institutional Retirement Reporting Team / Agent: Verifies that the dataset scope matches the intended reporting scope (e.g., if certain
portfolios were meant to be included/excluded) and that the outputs can roll up to the level needed for disclosure (for instance, if they will report
at an aggregate institutional level, ensure all relevant assets are included exactly once).
• 6. Group Climate Team / Agent: Checks the dataset against regulatory expectations: for instance, if the TCFD report requires year-on-year
changes, do we have last year’s data to compare; if required to disclose data quality or methodologies, is the info recorded. They might also ensure
that scenario analysis data (outside this dataset) remains consistent (though scenario analysis is separate, no conflicting numbers).
• Address any feedback: e.g., if a proxy seems too high-level, perhaps refine it; if an asset should be classified differently, adjust that classification.

Outputs:

• Output from 6 Validated and approved dataset (final input for calculations). At this point, we have effectively a green-light to proceed to
emission calculations, with confidence in the data. All investments have emissions data (actual or estimated) and all inputs are vetted

3. Emission calculation

Sub-process 3.1: Calculate Emissions for Corporate Bonds
Inputs

• Internal data sources: inputs come directly from the validated dataset of Phase 2

Processing Logic:

• For corporate bonds and listed equity holdings we have: the investment value (e.g. amount of bond holding), the enterprise value (EVIC) of the issuer,
and the issuer’s GHG emissions (Scope 1 and 2). These inputs come directly from the validated dataset of Phase 2
• Method & Processing Logic: We apply the PCAF attributed emissions formula. For each corporate issuer::

• For example, if we own 1% of a company (by value), we assume responsibility for 1% of that company’s emissions
• We ensure to use the same emissions figure for the company whether we hold equity or debt, to avoid double counting. If the firm has multiple
securities in the portfolio (e.g., we hold both bond and equity of Company X), we calculate emissions for each holding and then sum them – effectively
still getting (bond value + equity value)/EV * emissions, which is total exposure fraction times emissions (same result as if done once combined). We need
to be careful not to count the company twice in aggregate results.
• If any company emissions include Scope 3 (not typical unless explicitly intended), we would handle them separately to avoid mixing scopes. In general,
we stick to Scope 1 and 2 for the financed emissions footprint as per common practice. However, in this case L&G wants to see Scope 3 in the final output
• For sovereign bonds (not explicitly mentioned, but if any government bonds were present, PCAF has a different method. For this mock up
• The calculation is performed programmatically for all relevant rows. After computing each, we sum up emissions by issuer, by sector, etc., as needed.
• Outputs: Financed emissions for each corporate issuer investment, typically expressed in tons CO₂e.
• We can aggregate these at various levels for reporting: e.g., total financed emissions from corporate bonds, total from listed equities (if any), and
combined. We also compute an emissions intensity metric for this segment, such as tons CO₂e per £ million invested, to facilitate comparisons and to
meet any disclosure requirement for intensity. For instance, if our total financed emissions in this category is 50,000 tCO₂e on £5,000M invested, the
intensity is 10 tCO₂e/£m. Another metric is Weighted Average Carbon Intensity (WACI), but that’s a different calculation (often required by TCFD, but
WACI uses emissions/ revenue weighted by portfolio weight – which could be computed separately if needed). The primary output is absolute emissions

3. Emission calculation

Sub-process 3.2: Calculate Emissions for Real Estate Equity
Inputs

• Internal data sources: inputs come directly from the validated dataset of Phase 2

Processing Logic:

• For each property asset, we have either actual operational emissions data or an estimated emissions figure (from Phase 2, likely calculated via energy
use or proxy). We also have our ownership share in the asset (often 100% if wholly owned, or some percentage if co-owned). In lieu of EVIC (since a
building isn’t a company with debt/equity split in the same way), the attribution factor is the ownership percentage or financed fraction of the asset’s
value.
• Method & Processing Logic: Formally, for each property p:

• The Property Emissions would ideally be the sum of Scope 1 and 2 emissions from building operations (e.g., fuel combustion on-site, purchased
electricity) over a year. We ensure those emissions are either measured (from utility data, fuel usage) or estimated via the proxies applied earlier. If
properties are held via a property company or fund structure, but essentially we have look-through to the asset, we use the same approach.
• For each property, if we had to compute emissions via energy, we do: Energy use (kWh) × carbon factor (kgCO₂e/kWh) for each energy type, summing to
total tCO₂e3. (This would have been done in data acquisition or validation step for proxies; here we just use the result).
• No additional financial factor like EVIC is needed since we’re directly allocating by ownership. The “investment value” in a sense is proportional to
ownership anyway. If needed, we could also check consistency by taking (our investment value / property value) as the fraction – it should equal our
ownership share. If not, that implies perhaps leverage on the property; however, since we are equity owners, PCAF would treat it as if we finance that
share of emissions fully.
• We aggregate emissions across all real estate assets

• Outputs: Financed emissions for each real estate investment, typically expressed in tons CO₂e.

• Financed emissions from real estate portfolio (tCO₂e). This could be broken down by property or summed. Likely we’ll report an aggregate figure for real
estate investments. Additional outputs might include the breakdown by type of emissions (e.g., electricity vs gas usage contributions) if needed
internally, and an intensity metric like kg CO₂e per square meter across the portfolio to align with any real estate benchmarks
• for external disclosure, usually the total financed emissions attributed to real estate holdings is the key number. We again record the PCAF data quality
(perhaps many property emissions might be estimated if direct data was unavailable). If any properties were only partially owned, that is already

3. Emission calculation

Sub-process 3.3: Calculate Emissions for Infrastructure Equity
Inputs

• Internal data sources: inputs come directly from the validated dataset of Phase 2

Processing Logic:

• For each project, we should have an emissions figure (either reported by the project operator or estimated via proxies, depending on data availability
from Phase 2) and our ownership percentage or share of financing. If the infrastructure investment is through a fund, we may have to look through to
underlying projects proportionally.
• Method & Processing Logic: PCAF’s project finance methodology is similar to bonds : allocate based on share of total project cost financed. For each
project j:

• This mirrors the approach for real estate, using share of equity or loan in the project as attribution. If we provided, say, 20% of the capital for an
infrastructure project, we take responsibility for 20% of its annual emissions.
• Project Emissions should cover operational emissions of the infrastructure. For example, if it’s a renewable energy project, operational emissions might
be low (mostly maintenance vehicles, etc.), whereas a transportation infrastructure (like a toll road) might have more significant operational footprint
(energy use for lighting, etc.). If it’s a power generation project (e.g., a gas power plant), the combustion emissions would be the major contributor. We
use whatever data is available; if none, we use industry proxy (like emissions per MW for similar plants, etc.).
• Ensure that any debt vs equity distinction in project finance is considered: if our investment is equity and the project also has loans, PCAF would still have
us count our portion of emissions based on total project value financed. In practice, since we’re likely calculating for “infrastructure equity”, we assume
equity share as the proportion. (If there were project loans we provided, it would be similar calculation treating loan as part of financing).
• Compute for each project and sum up.

• Outputs: Financed emissions for each infrastructure equity investment (project finance), typically expressed in
tons CO₂e

• Financed emissions from infrastructure investments (tCO₂e). Again, aggregated as needed (total for this category). We might also note project-level
results internally, especially if we want to identify high emission projects vs low (e.g., a list: Project A – 1,000 tCO₂e, Project B – 0 tCO₂e if it’s a wind farm,
etc.). This could feed into future strategy (which projects to decarbonise)
• For reporting, likely the total figure is disclosed, possibly with qualitative commentary on the nature of those assets. Intensity metrics here are less

Scope 3 insight and decision
making

Scope 3 as Part of Sustainable Investment, Climate Risk
Management and Portfolio Management Decision Making Candidates (Illustrative)
Hypothesis (testable
statement)

Key Data Inputs

Modelling / Analytics Required

Climate adjusted financial metrics (e.g.
emissions-adjusted yield curves): Using
corporate bonds asasn example, issuers of such
bonds with high Scope 3 intensity will see
steeper yield curves due to anticipated transition
risk premia.

Bond yield curves by issuer
- Scope 3 intensity (tCO₂e / $ revenue)
- Sector decarbonisation pathways
- Carbon price forecasts

- Regression of yield spreads vs Scope 3 intensity
controlling for credit risk
- Scenario analysis on carbon pricing impacts on
refinancing costs
- Build adjusted yield curves with risk premia

Portfolio Value-at-Risk (VaR) uplift from Scope
3 transition scenarios: Integrating Scope 3
transition shocks increases portfolio VaR, leading
to allocation shifts.

- Portfolio holdings
- Issuer Scope 1–3 data
- Transition risk scenarios (NGFS)
- Sector elasticity to carbon prices

- Climate stress testing (e.g., PACTA, MSCI CVaR)
with Scope 3 included
- Monte Carlo simulation of carbon price shocks
- Sensitivity analysis of VaR by sector

Physical risk correlation with Scope 3
hotspots: Assets in sectors with high Scope 3
also face higher physical risk to operations/value
chains, raising default risk.

- Physical risk maps (flood, heat, drought)
- Supplier geolocation data
- Scope 3 category breakdown

- Overlay Scope 3 upstream geography with
climate hazard layers
- Bayesian network linking supply disruption to
credit risk
- Correlation analysis between hazard exposure
and Scope 3 intensity

Sector rotation strategy based on Scope 3
abatement cost curves: Shifting portfolio
weight to sectors where marginal abatement
cost (MAC) of Scope 3 is lowest generates

- MAC curves by sector
- Scope 3 baselines
- Policy tightening probability

- Optimisation: maximise expected return subject
to carbon budget
- Game-theoretic modelling of sector
decarbonisation

Climate adjusted financial metrics - from George to build and
layer
Cash Flows – Top Down
Traditional Climate Risk Measurement
is a top-down exercise

Issuer

Measure Type

Yield

Total

2025

2026

2027

2028

2029

2030

2031

2032

2033

2034

2035

2036

2037

2038

2039

2040

Traditional
Climate RCP 2.6
Climate RCP 4.5
Climate RCP 8.0

AI enables granular estimates of climate and
transition hazards by geography and asset –
even if the data is not readily available
Leverage AI to generate
granular inventory of
issuer assets and revenue
generating entities

Assets

Map assets to
location specific
physical and
transition risks

Offices

Aggregate to create firm
level view of climate risk

Physical Risks
Asset Name

Transition Risks
Litigation
Energy Costs

Plants
Policy
Supply Chain
Raw Materials
Leverage AI Data
pipelines to ingest
climate data and
damage functions

Cash Flows – Bottom Up

Technology
Reputation

Climate Models &
Damage Functions

Measure Type

Office

Climate RCP 2.6

Plant 1

Climate RCP 2.6

Plant 2

Climate RCP 2.6

Raw Material A

Climate RCP 2.6

Office

Climate RCP 4.5

Plant 1

Climate RCP 4.5

Plant 2

Climate RCP 4.5

Raw Material A

Climate RCP 4.5

Office

Climate RCP 8.0

Plant 1

Climate RCP 8.0

Plant 2

Climate RCP 8.0

Raw Material A

Climate RCP 8.0

Yield

Total

2025

2026

2027

2028

2029

2030

2031

2032

2033

2034

Gareth’s Team
Modelling
Calculations

Data From
IM

Data sent to
group

Validation
Directly Sourced
Data

Transformation

Orchestration
Enabling additional insights
Having highly paid/skilled
people working value add

Enables better insight from you
data
27

